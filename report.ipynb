{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情報システム論実習課題 テキスト分析\n",
    "\n",
    "## 利用する文書\n",
    "様々な国のWikipediaにおけるabstractを取り出したデータセット．\n",
    "\n",
    "## 利用する表現手法\n",
    "\n",
    "ここでは利用する表現手法として，BoWとTF-IDFを利用する．\n",
    "\n",
    "## 利用する距離\n",
    "BoWについては，マンハッタン距離とユークリッド距離，コサイン類似度によって計算する．\n",
    "TF-IDFについては，コサイン類似度を計算する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際の処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なパッケージのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtwisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mtwisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q nltk\n",
    "!pip install -q gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なパッケージのインポート・ダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/restartsugar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/restartsugar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/restartsugar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn #lemmatize関数のためのimport\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データのデータフレームへの格納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Japan is an island country in East Asia. Locat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>The United States of America (USA), commonly k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>England</td>\n",
       "      <td>England is a country that is part of the Unite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>China</td>\n",
       "      <td>China, officially the People's Republic of Chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India</td>\n",
       "      <td>India, also known as the Republic of India,[19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Korea is a region in East Asia.[3] Since 1948 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Germany, officially the Federal Republic of Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Russia</td>\n",
       "      <td>Russia, or the Russian Federation[12], is a tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>France</td>\n",
       "      <td>France, officially the French Republic, is a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Italy, officially the Italian Republic,[10][11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Brazil officially the Federative Republic of B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Canada is a country in the northern part of No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Spain, officially the Kingdom of Spain[11][a][...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia, officially the Commonwealth of Aust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Indonesia, officially the Republic of Indonesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>Mexico, officially the United Mexican States (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name                                           Abstract\n",
       "0           Japan  Japan is an island country in East Asia. Locat...\n",
       "1   United States  The United States of America (USA), commonly k...\n",
       "2         England  England is a country that is part of the Unite...\n",
       "3           China  China, officially the People's Republic of Chi...\n",
       "4           India  India, also known as the Republic of India,[19...\n",
       "5           Korea  Korea is a region in East Asia.[3] Since 1948 ...\n",
       "6         Germany  Germany, officially the Federal Republic of Ge...\n",
       "7          Russia  Russia, or the Russian Federation[12], is a tr...\n",
       "8          France  France, officially the French Republic, is a c...\n",
       "9           Italy  Italy, officially the Italian Republic,[10][11...\n",
       "10         Brazil  Brazil officially the Federative Republic of B...\n",
       "11         Canada  Canada is a country in the northern part of No...\n",
       "12          Spain  Spain, officially the Kingdom of Spain[11][a][...\n",
       "13      Australia  Australia, officially the Commonwealth of Aust...\n",
       "14      Indonesia  Indonesia, officially the Republic of Indonesi...\n",
       "15         Mexico  Mexico, officially the United Mexican States (..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./nlp_country.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop wordsで冠詞、代名詞、前置詞など一般的な語を削除するための関数を定義する．今回は英語であるので英語のstop wordsのリストを利用する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(word, stopwordset):\n",
    "  if word in stopwordset:\n",
    "    return None\n",
    "  else:\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的に，前処理をしていく．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "  def cleaning_text(text):\n",
    "    # @の削除\n",
    "    pattern1 = '@|%'\n",
    "    text = re.sub(pattern1, '', text)    \n",
    "    pattern2 = '\\[[0-9 ]*\\]'\n",
    "    text = re.sub(pattern2, '', text)    \n",
    "    # <b>タグの削除\n",
    "    pattern3 = '\\([a-z ]*\\)'\n",
    "    text = re.sub(pattern3, '', text)    \n",
    "    pattern4 = '[0-9]'\n",
    "    text = re.sub(pattern4, '', text)\n",
    "    return text\n",
    "  \n",
    "  def tokenize_text(text):\n",
    "    text = re.sub('[.,]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "  def lemmatize_word(word):\n",
    "    # make words lower  example: Python =>python\n",
    "    word=word.lower()\n",
    "    \n",
    "    # lemmatize  example: cooked=>cook\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "      return lemma\n",
    "    \n",
    "  text = cleaning_text(text)\n",
    "  tokens = tokenize_text(text)\n",
    "  tokens = [lemmatize_word(word) for word in tokens]\n",
    "  tokens = [remove_stopwords(word, en_stop) for word in tokens]\n",
    "  tokens = [word for word in tokens if word is not None]\n",
    "  return tokens\n",
    "  \n",
    "docs = df[\"Abstract\"].values\n",
    "pp_docs = [preprocessing_text(text) for text in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理したドキュメントで計算\n",
    "前処理したdocumentを用いて，BoWとTF-IDFを計算する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine類似度を計算する関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(list_a, list_b):\n",
    "  inner_prod = np.array(list_a).dot(np.array(list_b))\n",
    "  norm_a = np.linalg.norm(list_a)\n",
    "  norm_b = np.linalg.norm(list_b)\n",
    "  try:\n",
    "      return inner_prod / (norm_a*norm_b)\n",
    "  except ZeroDivisionError:\n",
    "      return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ミンコフスキー距離を計算する関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_distance(list_a, list_b, p):\n",
    "  diff_vec = np.array(list_a) - np.array(list_b)\n",
    "  return np.linalg.norm(x=diff_vec, ord=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "まずはBag of Wordsから計算する．bowのベクトルを計算する関数を定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_vectorizer(docs):\n",
    "  word2id = {}\n",
    "  for doc in docs:\n",
    "    for w in doc:\n",
    "      if w not in word2id:\n",
    "        word2id[w] = len(word2id)\n",
    "        \n",
    "  result_list = []\n",
    "  for doc in docs:\n",
    "    doc_vec = [0] * len(word2id)\n",
    "    for w in doc:\n",
    "      doc_vec[word2id[w]] += 1\n",
    "    result_list.append(doc_vec)\n",
    "  return result_list, word2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に計算する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vec, word2id = bow_vectorizer(pp_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は日本のドキュメントと最もよく似ている国を示す．まず，コサイン類似度による計算を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW: cosine類似度\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 1.0000000000000002,\n",
       " 1: 0.30868394472237093,\n",
       " 2: 0.24442109959824154,\n",
       " 3: 0.31380271433857343,\n",
       " 4: 0.24479467332586202,\n",
       " 5: 0.34372825633151943,\n",
       " 6: 0.3234461542498459,\n",
       " 7: 0.30413196719287744,\n",
       " 8: 0.33713265112489266,\n",
       " 9: 0.3270549101118192,\n",
       " 10: 0.2846409928828126,\n",
       " 11: 0.22429633498453364,\n",
       " 12: 0.2846361894175649,\n",
       " 13: 0.3037161720774084,\n",
       " 14: 0.3871304096992261,\n",
       " 15: 0.3196186379883876}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_cosine(vector, vector_list):\n",
    "  result = {}\n",
    "  for i, x in enumerate(vector_list):\n",
    "    result[i] = cosine_similarity(vector, vector_list[i])\n",
    "    \n",
    "  return result\n",
    "\n",
    "print(\"BoW: cosine類似度\")\n",
    "res = calc_cosine(bow_vec[0],bow_vec)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果から日本を除いて最も数値が大きいのは14の国であり，その国はインドネシアである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に日本のドキュメントと各ドキュメントのミンコフスキー距離を計算するための関数を示す．\n",
    "今回はミンコフスキー距離の$p$は，$p=1$， $p=2$の2通りで計算する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_minkowski(vector, vector_list, p):\n",
    "    result = {}\n",
    "    result = {}\n",
    "    for i, x in enumerate(vector_list):\n",
    "        result[i] = minkowski_distance(vector, x, p)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マンハッタン距離の場合($p=1$)を示す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW: マンハッタン距離\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.0,\n",
       " 1: 595.0,\n",
       " 2: 474.0,\n",
       " 3: 503.0,\n",
       " 4: 467.0,\n",
       " 5: 528.0,\n",
       " 6: 512.0,\n",
       " 7: 614.0,\n",
       " 8: 522.0,\n",
       " 9: 597.0,\n",
       " 10: 496.0,\n",
       " 11: 513.0,\n",
       " 12: 579.0,\n",
       " 13: 478.0,\n",
       " 14: 505.0,\n",
       " 15: 504.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"BoW: マンハッタン距離\")\n",
    "res = ｃalc_minkowski(bow_vec[0],bow_vec, 1)\n",
    "res   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本を除くマンハッタン距離が最も小さいのは4でありその国はインドである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にユークリッド距離の場合を示す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW: ユークリッド距離\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.0,\n",
       " 1: 44.05678154382138,\n",
       " 2: 34.87119154832539,\n",
       " 3: 34.0147027033899,\n",
       " 4: 32.202484376209235,\n",
       " 5: 35.07135583350036,\n",
       " 6: 35.608987629529715,\n",
       " 7: 37.815340802378074,\n",
       " 8: 35.66510900025401,\n",
       " 9: 39.102429592034305,\n",
       " 10: 34.438350715445125,\n",
       " 11: 33.8673884437522,\n",
       " 12: 38.3275357934736,\n",
       " 13: 32.341923257592455,\n",
       " 14: 31.76476034853718,\n",
       " 15: 34.785054261852174}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"BoW: ユークリッド距離\")\n",
    "res = calc_minkowski(bow_vec[0],bow_vec, 2)\n",
    "res   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本を除くユークリッド距離が最も小さいのは4でありその国はインドである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果から，ユークリッド距離とcosine類似度の結果が一致しており，マンハッタン距離が異なるということが分かる．\n",
    "これはマンハッタン距離は単純な出現文字数の差を取る一方で，その二乗和をとってルートを取るという点で異なることからこのような結果の違いとして現れたと考える．また，cosine類似度はそのベクトルの角度が近いすなわち，文書の内容の方向性が近いものが抽出できる．こうした計算方法の違いで実際に近い文書として出力されることが結果からわかった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "つぎにTF-IDFを計算する．TF-IDFのベクトルを計算する関数を定義する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(docs):\n",
    "  def tf(word2id, doc):\n",
    "    term_counts = np.zeros(len(word2id))\n",
    "    for term in word2id.keys():\n",
    "      term_counts[word2id[term]] = doc.count(term)\n",
    "    tf_values = list(map(lambda x: x/sum(term_counts), term_counts))\n",
    "    return tf_values\n",
    "  \n",
    "  def idf(word2id, docs):\n",
    "    idf = np.zeros(len(word2id))\n",
    "    for term in word2id.keys():\n",
    "      idf[word2id[term]] = np.log(len(docs) / sum([bool(term in doc) for doc in docs]))\n",
    "    return idf\n",
    "  \n",
    "  word2id = {}\n",
    "  for doc in docs:\n",
    "    for w in doc:\n",
    "      if w not in word2id:\n",
    "        word2id[w] = len(word2id)\n",
    "  \n",
    "  return [[_tf*_idf for _tf, _idf in zip(tf(word2id, doc), idf(word2id, docs))] for doc in docs], word2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に前処理をした文書で計算する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec, word2id = tfidf_vectorizer(pp_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoWと同様，日本の文書と似ている文書を見つける．まずは，cosine類似度の計算をする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF: cosine類似度\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 1.0,\n",
       " 1: 0.04945156965230686,\n",
       " 2: 0.0355002685981015,\n",
       " 3: 0.07494324927746153,\n",
       " 4: 0.02200165046387345,\n",
       " 5: 0.089213868005443,\n",
       " 6: 0.04329186935344453,\n",
       " 7: 0.04340970910393382,\n",
       " 8: 0.05061679443369346,\n",
       " 9: 0.05446867547327852,\n",
       " 10: 0.03479541972998953,\n",
       " 11: 0.03392463518350004,\n",
       " 12: 0.038469390607195876,\n",
       " 13: 0.05035814117836253,\n",
       " 14: 0.06794378321355647,\n",
       " 15: 0.029516361108928312}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"TF-IDF: cosine類似度\")\n",
    "res = calc_cosine(tfidf_vec[0],tfidf_vec)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果から，最も近い文書は5の韓国であることが分かる．  \n",
    "次に，マンハッタン距離を求める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF: マンハッタン距離\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.0,\n",
       " 1: 2.3608735368449936,\n",
       " 2: 2.5731221468801357,\n",
       " 3: 2.6294207675543775,\n",
       " 4: 2.7744931615739383,\n",
       " 5: 2.6773565232088843,\n",
       " 6: 2.43907571391646,\n",
       " 7: 2.653363612428077,\n",
       " 8: 2.318776070485786,\n",
       " 9: 2.5387188371556677,\n",
       " 10: 2.45044237565666,\n",
       " 11: 2.5885343221688917,\n",
       " 12: 2.571199454754217,\n",
       " 13: 2.58376708137673,\n",
       " 14: 2.564644164768093,\n",
       " 15: 2.489820098650106}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"TF-IDF: マンハッタン距離\")\n",
    "res = ｃalc_minkowski(tfidf_vec[0],tfidf_vec, 1)\n",
    "res   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マンハッタン距離が最も小さいのは8のフランスであることが分かる．  \n",
    "次に，ユークリッド距離を求める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF: ユークリッド距離\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.0,\n",
       " 1: 0.13894063261041328,\n",
       " 2: 0.1954296993588011,\n",
       " 3: 0.16682345000092424,\n",
       " 4: 0.18428766675062896,\n",
       " 5: 0.18393038894598424,\n",
       " 6: 0.17064933882760383,\n",
       " 7: 0.1591377644349097,\n",
       " 8: 0.1489531710857635,\n",
       " 9: 0.16479906407130404,\n",
       " 10: 0.16829228129212442,\n",
       " 11: 0.1646238972305546,\n",
       " 12: 0.15311773054039643,\n",
       " 13: 0.1719659245803892,\n",
       " 14: 0.1530401348289774,\n",
       " 15: 0.16792101813795798}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"TF-IDF: ユークリッド距離\")\n",
    "res = ｃalc_minkowski(tfidf_vec[0],tfidf_vec, 2)\n",
    "res   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ユークリッド距離が最も小さいのは1のアメリカであることが分かる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考察\n",
    "TF-IDFの場合，結果が全ての距離尺度で異なる結果となった．また，BoWの結果と比較しても各尺度で一致している結果はなく，TF-IDFとBoWでは異なるアルゴリズムの結果が現れていると考えられる．一般的に用いられている尺度であるcosine類似度は韓国の文書が最も似ているという結果となった．TF-IDFは単語の出現頻度と，逆文書頻度を利用するため，単純な出現頻度を見る，BoWとは結果が異なると考える．また，利用する尺度や表現手法によってこうした結果の違いとして現れるため，利用する尺度は慎重に選定する必要があると考える．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感想\n",
    "利用する尺度，表現手法によって違いが実際に現れることが分かった．また，TF-IDFやBoWはコードを見ると案外簡単にかけることがわかった．一方難しいと感じたのは，前処理の仕方であり，この部分がしっかりしていないと精度の高い文書比較は出来なさそうであると感じた．"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.5.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
